{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df3a3f36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\ssuni\\anaconda3\\lib\\site-packages (3.5.0)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\ssuni\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyspark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a6fcc9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler, OneHotEncoder\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark.sql.functions import when\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4336abc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"InjurySeverityPrediction\").getOrCreate()\n",
    "\n",
    "# Load data\n",
    "df = spark.read.csv(\"cleaned_traffic_crashes.csv\", header=True, inferSchema=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "73be1085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|     INJURY_SEVERITY|\n",
      "+--------------------+\n",
      "|Non-Incapacitatin...|\n",
      "|               Fatal|\n",
      "|Reported Not Evid...|\n",
      "|           No Injury|\n",
      "|Incapacitating In...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "df = df.withColumn(\"INJURY_SEVERITY\", \n",
    "                   when(df[\"MOST_SEVERE_INJURY\"] == 'FATAL', 'Fatal')\n",
    "                   .when(df[\"MOST_SEVERE_INJURY\"] == 'INCAPACITATING INJURY', 'Incapacitating Injury')\n",
    "                   .when(df[\"MOST_SEVERE_INJURY\"] == 'NONINCAPACITATING INJURY', 'Non-Incapacitating Injury')\n",
    "                   .when(df[\"MOST_SEVERE_INJURY\"] == 'REPORTED, NOT EVIDENT', 'Reported Not Evident Injury')\n",
    "                   .otherwise('No Injury'))\n",
    "\n",
    "distinct_injury_severity = df.select(\"INJURY_SEVERITY\").distinct()\n",
    "distinct_injury_severity.show()\n",
    "\n",
    "binary_df = df.withColumn(\n",
    "    \"BINARY_INJURY_SEVERITY\",\n",
    "    when(\n",
    "        (df[\"INJURY_SEVERITY\"] == 'Fatal') |\n",
    "        (df[\"INJURY_SEVERITY\"] == 'Incapacitating Injury') |\n",
    "        (df[\"INJURY_SEVERITY\"] == 'Non-Incapacitating Injury') |\n",
    "        (df[\"INJURY_SEVERITY\"] == 'Reported Not Evident Injury'),\n",
    "        'Injury'\n",
    "    ).otherwise('No Injury')\n",
    ")\n",
    "#distinct_injury_severity = binary_df.select(\"BINARY_INJURY_SEVERITY\").distinct()\n",
    "#distinct_injury_severity.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eba77c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the features\n",
    "\n",
    "features = ['POSTED_SPEED_LIMIT', 'WEATHER_CONDITION', 'LIGHTING_CONDITION', \n",
    "            'TRAFFIC_CONTROL_DEVICE', 'DEVICE_CONDITION','ROADWAY_SURFACE_COND',\n",
    "            'ROAD_DEFECT','CRASH_TYPE']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d78d26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "indexers = [StringIndexer(inputCol=column, outputCol=column+\"_index\").fit(df) for column in features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "325cb720",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(inputCols=[indexer.getOutputCol() for indexer in indexers], outputCols=[col+\"_encoded\" for col in features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f88fe94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "assembler = VectorAssembler(inputCols=[col+\"_encoded\" for col in features], outputCol=\"features\")\n",
    "\n",
    "labelIndexer = StringIndexer(inputCol=\"INJURY_SEVERITY\", outputCol=\"label\").fit(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "baece630",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|            features|label|\n",
      "+--------------------+-----+\n",
      "|(98,[2,44,56,63,8...|  0.0|\n",
      "|(98,[0,44,56,63,8...|  0.0|\n",
      "|(98,[0,44,56,61,7...|  0.0|\n",
      "|(98,[2,44,56,60,8...|  0.0|\n",
      "|(98,[0,44,57,61,7...|  0.0|\n",
      "|(98,[0,44,56,60,7...|  0.0|\n",
      "|(98,[1,44,56,61,8...|  0.0|\n",
      "|(98,[5,44,56,60,7...|  0.0|\n",
      "|(98,[0,44,56,61,8...|  1.0|\n",
      "|(98,[0,44,56,61,7...|  0.0|\n",
      "|(98,[1,44,55,60,7...|  0.0|\n",
      "|(98,[0,44,56,61,7...|  0.0|\n",
      "|(98,[0,44,56,61,7...|  0.0|\n",
      "|(98,[3,44,57,61,7...|  0.0|\n",
      "|(98,[5,44,55,60,7...|  0.0|\n",
      "|(98,[0,44,56,60,7...|  0.0|\n",
      "|(98,[0,44,57,60,7...|  0.0|\n",
      "|(98,[0,44,57,60,7...|  3.0|\n",
      "|(98,[0,44,57,60,7...|  0.0|\n",
      "|(98,[0,44,56,60,7...|  0.0|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(stages=indexers + [encoder, assembler, labelIndexer])\n",
    "\n",
    "pipelineModel = pipeline.fit(df)\n",
    "transformed_df = pipelineModel.transform(df)\n",
    "\n",
    "transformed_df.select([\"features\", \"label\"]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eba01944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RF Accuracy = 0.8635177335889758\n",
      "RF Precision = 0.7456628762226413\n",
      "RF Recall = 0.8635177335889758\n",
      "RF F1-score = 0.8002745160750989\n"
     ]
    }
   ],
   "source": [
    "#1. Randomforestclassifier\n",
    "\n",
    "rf = RandomForestClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline(stages=indexers + [encoder, assembler, labelIndexer, rf])\n",
    "\n",
    "# Splitting the dataset\n",
    "(train, test) = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Evaluate the model\n",
    "rf_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "rf_accuracy = rf_evaluator.evaluate(predictions)\n",
    "rf_precision = rf_evaluator.evaluate(predictions, {rf_evaluator.metricName: \"weightedPrecision\"})\n",
    "rf_recall = rf_evaluator.evaluate(predictions, {rf_evaluator.metricName: \"weightedRecall\"})\n",
    "rf_f1 = rf_evaluator.evaluate(predictions, {rf_evaluator.metricName: \"f1\"})\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"RF Accuracy = \" + str(rf_accuracy))\n",
    "print(\"RF Precision = \" +str(rf_precision))\n",
    "print(\"RF Recall = \"+str(rf_recall))\n",
    "print(\"RF F1-score = \"+str(rf_f1))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b29e0509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Accuracy = 0.8633510102791333\n",
      "LR Precision = 0.8109876681324307\n",
      "LR Recall = 0.8633510102791332\n",
      "LR F1-score = 0.8235537512996971\n"
     ]
    }
   ],
   "source": [
    "# 2.LogisticRegressionClassifier\n",
    "\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Build the pipeline with all stages including the Logistic Regression classifier\n",
    "pipeline = Pipeline(stages=indexers + [encoder, assembler, labelIndexer, lr])\n",
    "\n",
    "# Splitting the dataset\n",
    "(train, test) = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Evaluate the model\n",
    "lr_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "lr_accuracy = lr_evaluator.evaluate(predictions)\n",
    "lr_precision = lr_evaluator.evaluate(predictions, {lr_evaluator.metricName: \"weightedPrecision\"})\n",
    "lr_recall = lr_evaluator.evaluate(predictions, {lr_evaluator.metricName: \"weightedRecall\"})\n",
    "lr_f1 = lr_evaluator.evaluate(predictions, {lr_evaluator.metricName: \"f1\"})\n",
    "\n",
    "print(\"LR Accuracy = \" + str(lr_accuracy))\n",
    "print(\"LR Precision = \" +str(lr_precision))\n",
    "print(\"LR Recall = \"+str(lr_recall))\n",
    "print(\"LR F1-score = \"+str(lr_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "99fc9595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DT Accuracy = 0.8637614061187455\n",
      "DT Precision = 0.7865504827202008\n",
      "DT Recall = 0.8637614061187455\n",
      "DT F1-score = 0.8015812928742783\n"
     ]
    }
   ],
   "source": [
    "# 3. DecisionTreeClassifier\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier,DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "dt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# Build the pipeline with all stages including the Decision Tree classifier\n",
    "pipeline = Pipeline(stages=indexers + [encoder, assembler, labelIndexer, dt])\n",
    "\n",
    "# Splitting the dataset\n",
    "(train, test) = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train)\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.transform(test)\n",
    "\n",
    "# Evaluate the model\n",
    "dt_evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "dt_accuracy = dt_evaluator.evaluate(predictions)\n",
    "dt_precision = dt_evaluator.evaluate(predictions, {dt_evaluator.metricName: \"weightedPrecision\"})\n",
    "dt_recall = dt_evaluator.evaluate(predictions, {dt_evaluator.metricName: \"weightedRecall\"})\n",
    "dt_f1 = dt_evaluator.evaluate(predictions, {dt_evaluator.metricName: \"f1\"})\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"DT Accuracy = \" + str(dt_accuracy))\n",
    "print(\"DT Precision = \" +str(dt_precision))\n",
    "print(\"DT Recall = \"+str(dt_recall))\n",
    "print(\"DT F1-score = \"+str(dt_f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67513332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC Accuracy = 0.8875451275112699\n",
      "SVC Precision = 0.8866783678936732\n",
      "SVC Recall = 0.8875451275112698\n",
      "SVC F1 Score = 0.8871031098846179\n"
     ]
    }
   ],
   "source": [
    "## 4. LinearSVC classifier\n",
    "\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "from pyspark.ml.classification import LinearSVC\n",
    "\n",
    "\n",
    "binary_df = df.withColumn(\n",
    "    \"BINARY_INJURY_SEVERITY\",\n",
    "    when(\n",
    "        (df[\"INJURY_SEVERITY\"] == 'Fatal') |\n",
    "        (df[\"INJURY_SEVERITY\"] == 'Incapacitating Injury') |\n",
    "        (df[\"INJURY_SEVERITY\"] == 'Non-Incapacitating Injury') |\n",
    "        (df[\"INJURY_SEVERITY\"] == 'Reported Not Evident Injury'),\n",
    "        'Injury'\n",
    "    ).otherwise('No Injury')\n",
    ")\n",
    "\n",
    "binary_labelIndexer = StringIndexer(inputCol=\"BINARY_INJURY_SEVERITY\", outputCol=\"label\").fit(binary_df)\n",
    "\n",
    "svc = LinearSVC(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "pipeline_svc = Pipeline(stages=indexers + [encoder, assembler, binary_labelIndexer, svc])\n",
    "\n",
    "(train_svc, test_svc) = binary_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "model_svc = pipeline_svc.fit(train_svc)\n",
    "\n",
    "predictions_svc = model_svc.transform(test_svc)\n",
    "\n",
    "binary_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"prediction\", metricName=\"areaUnderROC\")\n",
    "svc_auc = binary_evaluator.evaluate(predictions_svc)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "f1 = evaluator.evaluate(predictions_svc, {evaluator.metricName: \"f1\"})\n",
    "weightedRecall = evaluator.evaluate(predictions_svc, {evaluator.metricName: \"weightedRecall\"})\n",
    "weightedPrecision = evaluator.evaluate(predictions_svc, {evaluator.metricName: \"weightedPrecision\"})\n",
    "accuracy = evaluator.evaluate(predictions_svc, {evaluator.metricName: \"accuracy\"})\n",
    "\n",
    "# Print metrics\n",
    "#print(\"SVC AUC = \" + str(svc_auc))\n",
    "print(\"SVC Accuracy = \" + str(accuracy))\n",
    "print(\"SVC Precision = \" + str(weightedPrecision))\n",
    "print(\"SVC Recall = \" + str(weightedRecall))\n",
    "print(\"SVC F1 Score = \" + str(f1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da4d2848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GBT Accuracy = 0.8895842818393429\n",
      "GBT Precision = 0.9038458245421899\n",
      "GBT Recall = 0.8895842818393428\n",
      "GBT F1 Score = 0.8950201892133081\n"
     ]
    }
   ],
   "source": [
    "# 5. Gradient boosting classifier\n",
    "\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "gbt = GBTClassifier(labelCol=\"label\", featuresCol=\"features\", maxIter=10)\n",
    "\n",
    "pipeline_gbt = Pipeline(stages=indexers + [encoder, assembler, binary_labelIndexer, gbt])\n",
    "\n",
    "(train_gbt, test_gbt) = binary_df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "model_gbt = pipeline_gbt.fit(train_gbt)\n",
    "\n",
    "predictions_gbt = model_gbt.transform(test_gbt)\n",
    "\n",
    "gbt_evaluator = BinaryClassificationEvaluator(labelCol=\"label\", rawPredictionCol=\"probability\", metricName=\"areaUnderROC\")\n",
    "gbt_auc = gbt_evaluator.evaluate(predictions_gbt)\n",
    "\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\")\n",
    "f1 = evaluator.evaluate(predictions_gbt, {evaluator.metricName: \"f1\"})\n",
    "weightedRecall = evaluator.evaluate(predictions_gbt, {evaluator.metricName: \"weightedRecall\"})\n",
    "weightedPrecision = evaluator.evaluate(predictions_gbt, {evaluator.metricName: \"weightedPrecision\"})\n",
    "accuracy = evaluator.evaluate(predictions_gbt, {evaluator.metricName: \"accuracy\"})\n",
    "\n",
    "#print(\"GBT AUC = \" + str(gbt_auc))\n",
    "print(\"GBT Accuracy = \" + str(accuracy))\n",
    "print(\"GBT Precision = \" + str(weightedPrecision))\n",
    "print(\"GBT Recall = \" + str(weightedRecall))\n",
    "print(\"GBT F1 Score = \" + str(f1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1319b915",
   "metadata": {},
   "source": [
    "Accuracy: The Gradient Boosting Classifier (GBT) has the highest accuracy (0.8896), indicating it correctly classifies the highest percentage of instances among the models tested.\n",
    "\n",
    "Precision and Recall: GBT also leads in precision (0.9038), suggesting it has the lowest false positive rate, and has a high recall (0.8896), indicating it correctly identifies a high percentage of actual positives.\n",
    "\n",
    "F1 Score: The F1 score is the harmonic mean of precision and recall, providing a balance between the two. GBT again has the highest F1 score (0.8950), suggesting it has the most balanced performance between precision and recall among the models evaluated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b98b18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda1a613",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de35418",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
